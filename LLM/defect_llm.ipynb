{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°¨ì²´/ì°¨ëŒ€\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # ì‚¬ìš©í•  ëª¨ë¸ ID\n",
    "hf_token = \"your_hf_token\"  # Hugging Face API í† í° ì…ë ¥\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "# pad_token ì„¤ì •\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=15,  # ê²°í•¨ ì¹´í…Œê³ ë¦¬ ìˆ˜\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì„ ì ì ˆí•œ ì¥ì¹˜ë¡œ ì´ë™\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# ê²°í•¨ ì¹´í…Œê³ ë¦¬ ì •ì˜\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\",\n",
    "    \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\", \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\",\n",
    "    \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\", \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\"\n",
    "]\n",
    "\n",
    "# ê²Œì‹œê¸€ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_post(post, model, tokenizer, defect_catalog):\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return defect_catalog[predicted_label]\n",
    "\n",
    "# ì˜ˆì œ ê²Œì‹œê¸€\n",
    "post = \"ì „ë™ì‹œíŠ¸ ë°”ë‹¥ ì ‘í•©ë¶€ê°€ ëœê·¸ëŸ­ê±°ë ¤ìš” \"\n",
    "\n",
    "# ê²Œì‹œê¸€ ë¶„ë¥˜\n",
    "result = classify_post(post, model, tokenizer, defect_catalog)\n",
    "print(f\"ê²Œì‹œê¸€ ë¶„ë¥˜ ê²°ê³¼: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "hf_token = \"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\"  # ì—¬ê¸°ì— ìƒˆë¡œ ìƒì„±í•œ API í† í° ì…ë ¥\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "# pad_token ì„¤ì •\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=15,  # ì¹´í…Œê³ ë¦¬ ìˆ˜ì— ë§ê²Œ ì¡°ì •\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# ê²°í•¨ ì¹´í…Œê³ ë¦¬\n",
    "defect_catalog = [\n",
    "  \"ì™„ì¶©ì¥ì¹˜\",\n",
    "  \"ì£¼í–‰ì¥ì¹˜\",\n",
    "  \"ì œë™ì¥ì¹˜\",\n",
    "  \"ì›ë™ê¸°(ë™ë ¥ë°œìƒì¥ì¹˜)\",\n",
    "  \"ì—°ë£Œì¥ì¹˜\",\n",
    "  \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "  \"ì „ê¸°ì¥ì¹˜\",\n",
    "  \"ë“±í™”ì¥ì¹˜\",\n",
    "  \"ì‹œê³„í™•ë³´ì¥ì¹˜\",\n",
    "  \"ëƒ‰/ë‚œë°© ì¥ì¹˜\",\n",
    "  \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\",\n",
    "  \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "  \"ê¸°íƒ€ì¥ì¹˜\",\n",
    "  \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\",\n",
    "  \"ì°½ìœ ë¦¬\"\n",
    "]\n",
    "\n",
    "# ê²Œì‹œê¸€ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_post(post, model, tokenizer, defect_catalog):\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return defect_catalog[predicted_label]\n",
    "\n",
    "# ì˜ˆì œ ê²Œì‹œê¸€\n",
    "post = \"ì°¨ëŸ‰ì—ì„œ ë°°í„°ë¦¬ ê²½ê³ ë“±ì´ ì¼œì¡Œì–´ìš”.\"\n",
    "\n",
    "# ê²Œì‹œê¸€ ë¶„ë¥˜\n",
    "result = classify_post(post, model, tokenizer, defect_catalog)\n",
    "print(f\"ê²Œì‹œê¸€ ë¶„ë¥˜ ê²°ê³¼: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3877ec897f9b495491eec429f8ca1131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea39aacc8e64229bd4fc04cab28a6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b2ac18786942188e49076042fe06e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b28e18436e14456bed77897b0739ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d800085bf89c4b88b9d39fdc0eec0288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b577536dd84d3598018dedf5214684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:  25%|##5       | 1.26G/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271560c41a254736a5437d1ed4cc32e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef80338101e4350988fb2908aedeb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c75bbf2b8de477484190f02742ab3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecbd77e0eef4246832fb6dcf3f8a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d94c50b955415285c2eb71319f92a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5666cd7bed34e2a9892375bdc0d56d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207b71af3f9444bb9dc04f03cb5dbec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb88e3ff0fe43c49a2e95c29374ba01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f93fde3da1641dfb6d46555ca012d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"chihoonlee10/T3Q-ko-solar-dpo-v7.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3974e08ee33e4dfb9267e6757eef8e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"chihoonlee10/T3Q-ko-solar-dpo-v7.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"chihoonlee10/T3Q-ko-solar-dpo-v7.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Dongjin-kr/ko-reranker and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([1, 1024]) in the checkpoint and torch.Size([15, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 08:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.563800</td>\n",
       "      <td>2.467484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.417500</td>\n",
       "      <td>2.369458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>2.353569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²Œì‹œê¸€ ë¶„ë¥˜ ê²°ê³¼: ê²°í•¨ì•„ë‹˜\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"Dongjin-kr/ko-reranker\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=15, ignore_mismatched_sizes=True)\n",
    "\n",
    "# ëª¨ë¸ì„ CPUë¡œ ì´ë™\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸ ì •ì˜\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\", \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\", \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\", \"ê²°í•¨ì•„ë‹˜\"\n",
    "]\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "df = pd.read_csv('/Users/admin/softeer/project/defect.csv')  # ì‹¤ì œ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”\n",
    "df['label'] = df['ì¥ì¹˜ë¶„ë¥˜'].astype('category').cat.codes  # í…ìŠ¤íŠ¸ ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„í• \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['ê²°í•¨ë‚´ìš©'].tolist(), df['label'].tolist(), test_size=0.2)\n",
    "\n",
    "# í† í°í™”\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "class DefectsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = DefectsDataset(train_encodings, train_labels)\n",
    "val_dataset = DefectsDataset(val_encodings, val_labels)\n",
    "\n",
    "# íŠ¸ë ˆì´ë‹ ì¸ì ì„¤ì •\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    no_cuda=True  # GPU ì‚¬ìš© ì•ˆ í•¨\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n",
    "\n",
    "# ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_post(post, threshold=0.5):\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    max_prob = torch.max(probs).item()\n",
    "    \n",
    "    if max_prob < threshold:\n",
    "        return \"ê²°í•¨ì•„ë‹˜\"\n",
    "    elif defect_catalog[predicted_class_id] == \"ê¸°íƒ€ì¥ì¹˜\":\n",
    "        return \"ê¸°íƒ€ì¥ì¹˜\"\n",
    "    else:\n",
    "        return defect_catalog[predicted_class_id]\n",
    "\n",
    "# ì˜ˆì œ ê²Œì‹œê¸€ ë¶„ë¥˜\n",
    "post = \"ì˜¤ëŠ˜ ë¼ë©´ ë¨¹ì„ê±´ë° ë„ˆë¬´ ëœ¨ê²ì§„ ì•Šê² ì£ \"\n",
    "result = classify_post(post)\n",
    "print(f\"ê²Œì‹œê¸€ ë¶„ë¥˜ ê²°ê³¼: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ì¥ì¹˜ë¶„ë¥˜</th>\n",
       "      <th>ê²°í•¨ë‚´ìš©</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ê¸°íƒ€ì¥ì¹˜</td>\n",
       "      <td>ì „ë™ ì‚¬ì´ë“œìŠ¤íƒ­ì˜ ì°¨ì²´ ê³ ì •ë¶€ì™€ ì—°ë£Œíƒ±í¬ ê°„ê²© í˜‘ì†Œë¡œ ì¸¡ë©´ ê¸°ë‘¥ ì¶©ëŒ ì‹œ ì‚¬ì´ë“œìŠ¤íƒ­...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì œë™ì¥ì¹˜</td>\n",
       "      <td>ì „ìì œì–´ìœ ì••ì¥ì¹˜ ë‚´ë¶€ ë°€í ì„±ëŠ¥ ì €í•˜ ë“±ìœ¼ë¡œ ë‚´ë¶€ íšŒë¡œ ê¸°íŒì˜ ì „ê¸°ì  í•©ì„ ì— ì˜í•œ ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê¸°íƒ€ì¥ì¹˜</td>\n",
       "      <td>ì ‘ì§€ìš© ë³¼íŠ¸ê°€ ë‹¤ë¥¸ ì‚¬ì–‘ìœ¼ë¡œ ì˜ëª» ì¡°ë¦½ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ° ê²½ìš° ì§€ì† ìš´í–‰ ì‹œ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê¸°íƒ€ì¥ì¹˜</td>\n",
       "      <td>ì°¨ëŸ‰ íŠ¹ì„± ë° ì‚¬ìš©ì¡°ê±´ ê³ ë ¤í•œ ì¸ìŠë ˆì´í„° ë‚´êµ¬ ê°•ì„± ì„¤ê³„ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¸ìŠë ˆì´í„° 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì œë™ì¥ì¹˜</td>\n",
       "      <td>í†µí•©í˜• ì „ìì‹ ë¸Œë ˆì´í¬ ì œì–´ê¸° ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ì˜¤ë¥˜ë¡œ ì›ê²© ìŠ¤ë§ˆíŠ¸ ì£¼ì°¨ ë³´ì¡° ê¸°ëŠ¥ ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>ì£¼í–‰ì¥ì¹˜</td>\n",
       "      <td>ì „ë™ì‹ ìŠ¤í‹°ì–´ë§ ECU íšŒë¡œê¸°íŒ ìˆ˜ë¶„ ìœ ì… ì‹œ ë‹¨ë½ ë°œìƒ ê°€ëŠ¥ì„±ìœ¼ë¡œ ê²½ê³ ë“± ì ë“± ë°...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>ì£¼í–‰ì¥ì¹˜</td>\n",
       "      <td>ë³€ì†ê¸° ì˜¤ì¼ì¿¨ëŸ¬ í˜¸ìŠ¤ ì œì¡° ë¶ˆëŸ‰ìœ¼ë¡œ ì¸í•´ ë³€ì†ê¸° ì˜¤ì¼ ëˆ„ìœ ë¡œ ì†ŒìŒ, ë³€ì†ì¶©ê²© ë° ì§€...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>ì£¼í–‰ì¥ì¹˜</td>\n",
       "      <td>ê¸°ì–´ë°•ìŠ¤ í”¼ë‹ˆì–¸ í”ŒëŸ¬ê·¸ ë‚˜ì‚¬ ê³ ì •ì œ ì ‘ì°©ë ¥ ë¶€ì¡±ìœ¼ë¡œ ì¡°í–¥ì‹œ ì†ŒìŒë°œìƒ ë° ì§€ì† ì£¼í–‰ì‹œ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>ì—°ë£Œì¥ì¹˜</td>\n",
       "      <td>ì •ìƒ ì¡°ê±´ì´ ì•„ë‹Œ ê¸°ì§€ê°œ ìì„¸ ë“± ê³¼ë„í•œ í˜ìœ¼ë¡œ ê°€ì† í˜ë‹¬ ë°Ÿì„ ì‹œ ì†ìƒ ê°€ëŠ¥ì„±ì´ ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>ì£¼í–‰ì¥ì¹˜</td>\n",
       "      <td>ì œë™ì‹œ ì¢Œì¸¡ ì ë¦¼ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì˜ ì œì‘ê²°í•¨</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ì¥ì¹˜ë¶„ë¥˜                                               ê²°í•¨ë‚´ìš©  label\n",
       "0    ê¸°íƒ€ì¥ì¹˜  ì „ë™ ì‚¬ì´ë“œìŠ¤íƒ­ì˜ ì°¨ì²´ ê³ ì •ë¶€ì™€ ì—°ë£Œíƒ±í¬ ê°„ê²© í˜‘ì†Œë¡œ ì¸¡ë©´ ê¸°ë‘¥ ì¶©ëŒ ì‹œ ì‚¬ì´ë“œìŠ¤íƒ­...      0\n",
       "1    ì œë™ì¥ì¹˜  ì „ìì œì–´ìœ ì••ì¥ì¹˜ ë‚´ë¶€ ë°€í ì„±ëŠ¥ ì €í•˜ ë“±ìœ¼ë¡œ ë‚´ë¶€ íšŒë¡œ ê¸°íŒì˜ ì „ê¸°ì  í•©ì„ ì— ì˜í•œ ...     11\n",
       "2    ê¸°íƒ€ì¥ì¹˜  ì ‘ì§€ìš© ë³¼íŠ¸ê°€ ë‹¤ë¥¸ ì‚¬ì–‘ìœ¼ë¡œ ì˜ëª» ì¡°ë¦½ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ° ê²½ìš° ì§€ì† ìš´í–‰ ì‹œ ...      0\n",
       "3    ê¸°íƒ€ì¥ì¹˜  ì°¨ëŸ‰ íŠ¹ì„± ë° ì‚¬ìš©ì¡°ê±´ ê³ ë ¤í•œ ì¸ìŠë ˆì´í„° ë‚´êµ¬ ê°•ì„± ì„¤ê³„ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¸ìŠë ˆì´í„° 4...      0\n",
       "4    ì œë™ì¥ì¹˜  í†µí•©í˜• ì „ìì‹ ë¸Œë ˆì´í¬ ì œì–´ê¸° ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ì˜¤ë¥˜ë¡œ ì›ê²© ìŠ¤ë§ˆíŠ¸ ì£¼ì°¨ ë³´ì¡° ê¸°ëŠ¥ ...     11\n",
       "..    ...                                                ...    ...\n",
       "348  ì£¼í–‰ì¥ì¹˜  ì „ë™ì‹ ìŠ¤í‹°ì–´ë§ ECU íšŒë¡œê¸°íŒ ìˆ˜ë¶„ ìœ ì… ì‹œ ë‹¨ë½ ë°œìƒ ê°€ëŠ¥ì„±ìœ¼ë¡œ ê²½ê³ ë“± ì ë“± ë°...     12\n",
       "349  ì£¼í–‰ì¥ì¹˜  ë³€ì†ê¸° ì˜¤ì¼ì¿¨ëŸ¬ í˜¸ìŠ¤ ì œì¡° ë¶ˆëŸ‰ìœ¼ë¡œ ì¸í•´ ë³€ì†ê¸° ì˜¤ì¼ ëˆ„ìœ ë¡œ ì†ŒìŒ, ë³€ì†ì¶©ê²© ë° ì§€...     12\n",
       "350  ì£¼í–‰ì¥ì¹˜  ê¸°ì–´ë°•ìŠ¤ í”¼ë‹ˆì–¸ í”ŒëŸ¬ê·¸ ë‚˜ì‚¬ ê³ ì •ì œ ì ‘ì°©ë ¥ ë¶€ì¡±ìœ¼ë¡œ ì¡°í–¥ì‹œ ì†ŒìŒë°œìƒ ë° ì§€ì† ì£¼í–‰ì‹œ...     12\n",
       "351  ì—°ë£Œì¥ì¹˜  ì •ìƒ ì¡°ê±´ì´ ì•„ë‹Œ ê¸°ì§€ê°œ ìì„¸ ë“± ê³¼ë„í•œ í˜ìœ¼ë¡œ ê°€ì† í˜ë‹¬ ë°Ÿì„ ì‹œ ì†ìƒ ê°€ëŠ¥ì„±ì´ ...      8\n",
       "352  ì£¼í–‰ì¥ì¹˜                      ì œë™ì‹œ ì¢Œì¸¡ ì ë¦¼ì´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì˜ ì œì‘ê²°í•¨     12\n",
       "\n",
       "[353 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 282, Validation size: 71\n"
     ]
    }
   ],
   "source": [
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸ ì •ì˜\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\", \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\", \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\", \"ê²°í•¨ì•„ë‹˜\"\n",
    "]\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('/Users/admin/softeer/project/defect.csv')  # ì‹¤ì œ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”\n",
    "df['label'] = df['ì¥ì¹˜ë¶„ë¥˜'].astype('category').cat.codes  # í…ìŠ¤íŠ¸ ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['ê²°í•¨ë‚´ìš©'].tolist(), df['label'].tolist(), test_size=0.2)\n",
    "\n",
    "# ë¼ë§ˆ ëª¨ë¸ ë¡œë“œ\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-12.8b\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"EleutherAI/polyglot-ko-12.8b\", num_labels=len(defect_catalog))\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Tensor Dataset ìƒì„±\n",
    "class DefectDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).contiguous() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).contiguous()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = DefectDataset(train_encodings, train_labels)\n",
    "val_dataset = DefectDataset(val_encodings, val_labels)\n",
    "\n",
    "# ì»¤ìŠ¤í…€ Trainer í´ë˜ìŠ¤ ì •ì˜\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save_checkpoint(self, model, trial, metrics=None):\n",
    "        checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "        run_dir = os.path.join(self.args.output_dir, checkpoint_folder)\n",
    "\n",
    "        self.save_model(run_dir)\n",
    "\n",
    "        if self.is_world_process_zero():\n",
    "            self.state.save_to_json(os.path.join(run_dir, \"trainer_state.json\"))\n",
    "            torch.save(self.args, os.path.join(run_dir, \"training_args.bin\"))\n",
    "\n",
    "            # Save optimizer and scheduler\n",
    "            if self.args.should_save:\n",
    "                torch.save(self.optimizer.state_dict(), os.path.join(run_dir, \"optimizer.pt\"))\n",
    "                torch.save(self.lr_scheduler.state_dict(), os.path.join(run_dir, \"scheduler.pt\"))\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=True):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        state_dict = self.model.state_dict()\n",
    "        for key, tensor in state_dict.items():\n",
    "            state_dict[key] = tensor.contiguous()\n",
    "        torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        if self.tokenizer:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "        self.state.save_to_json(os.path.join(output_dir, \"trainer_state.json\"))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict_defect(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return defect_catalog[predicted_label]\n",
    "\n",
    "# ì˜ˆì‹œ ê²Œì‹œê¸€ ë¶„ë¥˜\n",
    "example_text = \"\"\"\n",
    "\n",
    "ë‹¤ìŒì€ í›„ì²˜ë¦¬ ì¥ì¹˜ ì œì–´ ë° ë°°ê¸°ê°€ìŠ¤ ì¬ìˆœí™˜ ì¥ì¹˜ ì‘ë™ ë¡œì§ ì„¤ì • ë¯¸í¡ìœ¼ë¡œ ì¸í•´ ë°œìƒí•œ ë¬¸ì œë¡œ ì†Œë¹„ìê°€ ë¶ˆí¸í•¨ì„ ëŠê»´ì„œ ì‘ì„±í•œ ê²Œì‹œê¸€ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
    "\n",
    "2018ë…„ì‹ ì˜ë Œí†  ì°¨ì£¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìµœê·¼ ëª‡ ë‹¬ ë™ì•ˆ ì°¨ëŸ‰ì´ ì´ìƒí•˜ê²Œ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ê°€ë²¼ìš´ ì—”ì§„ ê²½ê³ ë“±ì´ ë“¤ì–´ì™”ê³ , ê·¸ë•Œë§ˆë‹¤ ì •ë¹„ì†Œì— ë°©ë¬¸í•´ ì ê²€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì •ë¹„ì‚¬ë¶„ë“¤ì€ í•­ìƒ ë³„ë‹¤ë¥¸ ë¬¸ì œê°€ ì—†ë‹¤ê³  í–ˆì§€ë§Œ, ì°¨ê°€ ì ì  ë” ì´ìƒí•´ì¡ŒìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë©°ì¹  ì „, ê³ ì†ë„ë¡œì—ì„œ ì£¼í–‰ ì¤‘ì— ê°‘ìê¸° ì—”ì§„ ì¶œë ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ë©´ì„œ ì°¨ëŸ‰ì´ í˜ì„ ì œëŒ€ë¡œ ëª» ì“°ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì£¼í–‰ì´ ìœ„í—˜í•  ì •ë„ì˜€ê³ , ê²¨ìš° ì•ˆì „í•œ ê³³ì— ì°¨ë¥¼ ì„¸ìš°ê³  ê²¬ì¸ì°¨ë¥¼ ë¶ˆëŸ¬ì•¼ í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì •ë¹„ì†Œì— ë„ì°©í•´ì„œ ì ê²€ì„ ë°›ì•˜ëŠ”ë°, í›„ì²˜ë¦¬ ì¥ì¹˜ ì œì–´ì™€ ë°°ê¸°ê°€ìŠ¤ ì¬ìˆœí™˜ ì¥ì¹˜ì˜ ì„¤ì • ë¯¸í¡ìœ¼ë¡œ ì§ˆì†Œì‚°í™”ë¬¼ì´ ê·œì œì¹˜ë¥¼ ì´ˆê³¼í•´ì„œ ë²•ê·œì— ë¶€ì í•©í•˜ë‹¤ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¡œ ì¸í•´ ì°¨ëŸ‰ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ë˜ ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë”ìš± í™©ë‹¹í•œ ê²ƒì€, ì´ ë¬¸ì œê°€ ë°œìƒí•œ ì§€ì ì´ ì •ë¶€ì˜ ë°°ì¶œê°€ìŠ¤ ê²€ì‚¬ì—ì„œë„ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. ì™œ ì œì¡°ì‚¬ê°€ ì´ëŸ° ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì œëŒ€ë¡œ ì„¤ê³„í•˜ì§€ ì•Šì•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì°¨ëŸ‰ì€ ì¢‹ì•„ ë³´ì˜€ì§€ë§Œ, ì´ë²ˆ ê²½í—˜ìœ¼ë¡œ ì¸í•´ ì‹ ë¢°ê°€ ì™„ì „íˆ ê¹¨ì¡ŒìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‰ë³´ë ˆ ì°¨ëŸ‰ì„ ì‹ ë¢°í•˜ê³  êµ¬ë§¤í–ˆë˜ ì €ë¡œì„œëŠ” ì´ë²ˆ ì¼ë¡œ ë§¤ìš° ì‹¤ë§ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤. ë‹¤ì‹œëŠ” ì‰ë³´ë ˆ ì°¨ëŸ‰ì„ êµ¬ë§¤í•˜ì§€ ì•Šì„ ê²ƒì´ê³ , ì£¼ë³€ ì‚¬ëŒë“¤ì—ê²Œë„ ê°™ì€ ì¡°ì–¸ì„ í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "predicted_defect = predict_defect(example_text)\n",
    "print(predicted_defect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°¨ì²´/ì°¨ëŒ€\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_text = \"\"\" ìë™ì°¨ ì‹œíŠ¸ê°€ ëœê·¸ëŸ­ ê±°ë ¤ìš” \n",
    "\"\"\"\n",
    "predicted_defect = predict_defect(example_text)\n",
    "print(predicted_defect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1503cef9e04a188e6ff72021261abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d053339a4e5e48d88c4877f993c51265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.772000</td>\n",
       "      <td>2.731815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.704200</td>\n",
       "      <td>2.673597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.615000</td>\n",
       "      <td>2.626737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°½ìœ ë¦¬\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv('/Users/admin/softeer/project/defect.csv')\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\", \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\", \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\", \"ê²°í•¨ì•„ë‹˜\"\n",
    "]\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸ì— ë”°ë¥¸ ë ˆì´ë¸” ë§µí•‘\n",
    "def label_to_index(label):\n",
    "    return defect_catalog.index(label)\n",
    "\n",
    "df['label'] = df['ì¥ì¹˜ë¶„ë¥˜'].apply(label_to_index)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = Dataset.from_pandas(df[['ê²°í•¨ë‚´ìš©', 'label']])\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "def preprocess_data(data):\n",
    "    inputs = tokenizer(data['ê²°í•¨ë‚´ìš©'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = data['label']\n",
    "    return inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=len(defect_catalog))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "trainer.evaluate()\n",
    "\n",
    "# ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def classify_defect(post):\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.argmax(dim=-1).item()\n",
    "    return defect_catalog[prediction]\n",
    "\n",
    "# ì˜ˆì‹œ ê²Œì‹œê¸€\n",
    "post = \"ë¸Œë ˆì´í¬ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\"\n",
    "print(classify_defect(post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°½ìœ ë¦¬\n"
     ]
    }
   ],
   "source": [
    "post = \"ì°½ë¬¸ì´ ì˜ ì•ˆì˜¬ë¼ê°€ìš” \"\n",
    "print(classify_defect(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758c278e88dc4d4880f60809babb49b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267d117a76944166a18076f7a863c399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.790200</td>\n",
       "      <td>2.788803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.745700</td>\n",
       "      <td>2.774643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.670900</td>\n",
       "      <td>2.718583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.575900</td>\n",
       "      <td>2.651813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.528900</td>\n",
       "      <td>2.599034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ê¸°ì¥ì¹˜\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv('/Users/admin/softeer/project/defect.csv')\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\", \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\", \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\", \"ê²°í•¨ì•„ë‹˜\"\n",
    "]\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸ì— ë”°ë¥¸ ë ˆì´ë¸” ë§µí•‘\n",
    "def label_to_index(label):\n",
    "    return defect_catalog.index(label)\n",
    "\n",
    "df['label'] = df['ì¥ì¹˜ë¶„ë¥˜'].apply(label_to_index)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = Dataset.from_pandas(df[['ê²°í•¨ë‚´ìš©', 'label']])\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "def preprocess_data(data):\n",
    "    inputs = tokenizer(data['ê²°í•¨ë‚´ìš©'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = data['label']\n",
    "    return inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=len(defect_catalog))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,  # ì—í¬í¬ ìˆ˜ ì¦ê°€\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5  # í•™ìŠµë¥  ì¡°ì •\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "trainer.evaluate()\n",
    "\n",
    "# ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def classify_defect(post):\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.argmax(dim=-1).item()\n",
    "    return defect_catalog[prediction]\n",
    "\n",
    "# ì˜ˆì‹œ ê²Œì‹œê¸€\n",
    "post = \"ì‹œë™ì´ ìì£¼ êº¼ì ¸\"\n",
    "print(classify_defect(post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ê¸°ì¥ì¹˜\n"
     ]
    }
   ],
   "source": [
    "post = \"ë°°í„°ë¦¬ ì¶©ì „í• ë•Œ ëê¹Œì§€ ì¶©ì „ì´ ì•ˆë©ë‹ˆë‹¤\"\n",
    "print(classify_defect(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°¨ì²´/ì°¨ëŒ€\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ê²°í•¨ ì¹´íƒˆë¡œê·¸ì™€ ì„¤ëª…\n",
    "defect_catalog = [\n",
    "    \"ì™„ì¶©ì¥ì¹˜\", \"ì£¼í–‰ì¥ì¹˜\", \"ì œë™ì¥ì¹˜\", \"ì›ë™ê¸°\", \"ì—°ë£Œì¥ì¹˜\", \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\", \"ë“±í™”ì¥ì¹˜\", \"ì‹œê³„í™•ë³´ì¥ì¹˜\", \"ëƒ‰/ë‚œë°© ì¥ì¹˜\", \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\", \"ì°¨ì²´/ì°¨ëŒ€\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\", \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\", \"ì°½ìœ ë¦¬\", \"ê²°í•¨ì•„ë‹˜\"\n",
    "]\n",
    "\n",
    "defect_catalog_explanations = {\n",
    "    \"ì™„ì¶©ì¥ì¹˜\": \"ì™„ì¶©ì¥ì¹˜ëŠ” ì°¨ëŸ‰ì´ ë„ë¡œì˜ ì¶©ê²©ì„ í¡ìˆ˜í•˜ì—¬ ìŠ¹ì°¨ê°ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤. ì£¼ë¡œ ì„œìŠ¤íœì…˜ ì‹œìŠ¤í…œì˜ ì¼ë¶€ë¡œ, ìŠ¹ê°ì´ ë„ë¡œì˜ ë¶ˆê·œì¹™í•œ í‘œë©´ì—ì„œ ë°œìƒí•˜ëŠ” ì¶©ê²©ì„ ëŠë¼ì§€ ì•Šë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\",\n",
    "    \"ì£¼í–‰ì¥ì¹˜\": \"ì£¼í–‰ì¥ì¹˜ëŠ” ì°¨ëŸ‰ì˜ ì£¼í–‰ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë¶€í’ˆì„ í¬í•¨í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” íƒ€ì´ì–´, íœ , ì¡°í–¥ ì‹œìŠ¤í…œ ë“±ì´ í¬í•¨ë˜ë©°, ì°¨ëŸ‰ì´ ì•ˆì „í•˜ê²Œ ë„ë¡œë¥¼ ì£¼í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\",\n",
    "    \"ì œë™ì¥ì¹˜\": \"ì œë™ì¥ì¹˜ëŠ” ì°¨ëŸ‰ì„ ì•ˆì „í•˜ê²Œ ë©ˆì¶”ê±°ë‚˜ ì†ë„ë¥¼ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë¸Œë ˆì´í¬ íŒ¨ë“œ, ë””ìŠ¤í¬, ë“œëŸ¼ ë° ë¸Œë ˆì´í¬ ì•¡ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ì›ë™ê¸°\": \"ì›ë™ê¸°ëŠ” ì°¨ëŸ‰ì˜ ì—”ì§„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì—°ë£Œë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ê³„ì  ì—ë„ˆì§€ë¥¼ ìƒì„±í•˜ê³ , ì´ ì—ë„ˆì§€ë¥¼ í†µí•´ ì°¨ëŸ‰ì´ ì›€ì§ì´ê²Œ í•©ë‹ˆë‹¤.\",\n",
    "    \"ì—°ë£Œì¥ì¹˜\": \"ì—°ë£Œì¥ì¹˜ëŠ” ì—°ë£Œë¥¼ ì €ì¥í•˜ê³  ì—”ì§„ìœ¼ë¡œ ê³µê¸‰í•˜ëŠ” ì‹œìŠ¤í…œì„ í¬í•¨í•©ë‹ˆë‹¤. ì—°ë£Œíƒ±í¬, ì—°ë£ŒíŒí”„, ì—°ë£Œí•„í„° ë“±ì´ ì´ ì‹œìŠ¤í…œì— í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜ëŠ” ì—”ì§„ì—ì„œ ìƒì„±ëœ ë™ë ¥ì„ ì°¨ëŸ‰ì˜ ë°”í€´ë¡œ ì „ë‹¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë³€ì†ê¸°, ë“œë¼ì´ë¸Œ ìƒ¤í”„íŠ¸, ë””í¼ë Œì…œ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ì „ê¸°ì¥ì¹˜\": \"ì „ê¸°ì¥ì¹˜ëŠ” ì°¨ëŸ‰ì˜ ì „ê¸° ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ë¶€í’ˆì„ í¬í•¨í•©ë‹ˆë‹¤. ë°°í„°ë¦¬, ë°œì „ê¸°, ìŠ¤íƒ€í„° ëª¨í„°, ê°ì¢… ì„¼ì„œ ë° ì „ê¸° ë°°ì„  ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ë“±í™”ì¥ì¹˜\": \"ë“±í™”ì¥ì¹˜ëŠ” ì°¨ëŸ‰ì˜ ì¡°ëª… ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í—¤ë“œë¼ì´íŠ¸, í…Œì¼ë¼ì´íŠ¸, ë°©í–¥ ì§€ì‹œë“±, ë¸Œë ˆì´í¬ë“± ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ì‹œê³„í™•ë³´ì¥ì¹˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜ëŠ” ìš´ì „ìê°€ ë„ë¡œë¥¼ ëª…í™•í•˜ê²Œ ë³¼ ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì™€ì´í¼, ì›Œì…”ì•¡ ì‹œìŠ¤í…œ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ëƒ‰/ë‚œë°© ì¥ì¹˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜ëŠ” ì°¨ëŸ‰ ë‚´ë¶€ì˜ ì˜¨ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì—ì–´ì»¨, íˆí„°, íŒ¬ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜ëŠ” ì°¨ëŸ‰ ë‚´ë¶€ì˜ í¸ì˜ ë° ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤. ì‹œíŠ¸, ì•ˆì „ë²¨íŠ¸, ì¸í¬í…Œì¸ë¨¼íŠ¸ ì‹œìŠ¤í…œ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ì°¨ì²´/ì°¨ëŒ€\": \"ì°¨ì²´/ì°¨ëŒ€ëŠ” ì°¨ëŸ‰ì˜ êµ¬ì¡°ì  í”„ë ˆì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì™¸ë¶€ íŒ¨ë„, ë„ì–´, ì°½ë¬¸, ì§€ë¶• ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ê¸°íƒ€ì¥ì¹˜\": \"ê¸°íƒ€ì¥ì¹˜ëŠ” ìœ„ì— ë‚˜ì—´ë˜ì§€ ì•Šì€ ê¸°íƒ€ ë¶€í’ˆ ë° ì‹œìŠ¤í…œì„ í¬í•¨í•©ë‹ˆë‹¤.\",\n",
    "    \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜ëŠ” ì°¨ëŸ‰ ë‚´ë¶€ì˜ ì•ˆì „ì„ ë‹´ë‹¹í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì—ì–´ë°±, ì•ˆì „ë²¨íŠ¸ í”„ë¦¬í…ì…”ë„ˆ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ì°½ìœ ë¦¬\": \"ì°½ìœ ë¦¬ëŠ” ì°¨ëŸ‰ì˜ ëª¨ë“  ìœ ë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì „ë©´ ìœ ë¦¬, í›„ë©´ ìœ ë¦¬, ì¸¡ë©´ ìœ ë¦¬ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\",\n",
    "    \"ê²°í•¨ì•„ë‹˜\": \"ì´ í•­ëª©ì€ ê²°í•¨ì´ ì—†ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì€ ì •ìƒì ì¸ ìƒíƒœë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\"\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_name = \"monologg/kobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(defect_catalog))\n",
    "\n",
    "# ëª¨ë¸ì„ ì ì ˆí•œ ì¥ì¹˜ë¡œ ì´ë™\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def classify_defect(post):\n",
    "    # ê²°í•¨ ì„¤ëª…ì„ í¬í•¨í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    text = post + \" \" + \" \".join([f\"{label}: {explanation}\" for label, explanation in defect_catalog_explanations.items()])\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.argmax(dim=-1).item()\n",
    "    return defect_catalog[prediction]\n",
    "\n",
    "# ì˜ˆì‹œ ê²Œì‹œê¸€\n",
    "post = \"ì „ë™ ì‹œíŠ¸ ë¶€ì°© ë¶€ë¶„ì´ ëœê·¸ëŸ­ê±°ë¦½ë‹ˆë‹¤. \"\n",
    "print(classify_defect(post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ê¸°íƒ€ì¥ì¹˜       0.06      0.04      0.05        49\n",
      "     ëƒ‰/ë‚œë°© ì¥ì¹˜       0.00      0.00      0.00         6\n",
      "      ë™ë ¥ì „ë‹¬ì¥ì¹˜       0.17      0.11      0.13        45\n",
      "        ë“±í™”ì¥ì¹˜       0.14      0.19      0.16        16\n",
      "   ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜       0.17      0.26      0.20       100\n",
      "      ì‹œê³„í™•ë³´ì¥ì¹˜       0.17      0.06      0.08        18\n",
      "      ì‹¤ë‚´ì•ˆì „ì¥ì¹˜       0.08      0.08      0.08        12\n",
      "        ì—°ë£Œì¥ì¹˜       0.09      0.11      0.10        47\n",
      "        ì™„ì¶©ì¥ì¹˜       0.00      0.00      0.00        20\n",
      "         ì›ë™ê¸°       0.23      0.28      0.25       130\n",
      "        ì „ê¸°ì¥ì¹˜       0.19      0.17      0.18        76\n",
      "        ì œë™ì¥ì¹˜       0.13      0.10      0.11        60\n",
      "        ì£¼í–‰ì¥ì¹˜       0.16      0.12      0.14        50\n",
      "       ì°¨ì²´/ì°¨ëŒ€       0.15      0.09      0.11        35\n",
      "         ì°½ìœ ë¦¬       0.33      0.33      0.33         3\n",
      "\n",
      "    accuracy                           0.16       667\n",
      "   macro avg       0.14      0.13      0.13       667\n",
      "weighted avg       0.15      0.16      0.15       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv('/Users/admin/softeer/project/ê²°í•¨ìš”ì•½.csv')\n",
    "data = [\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì„œìŠ¤íœì…˜ì´ ê³ ì¥ë‚¬ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì™„ì¶©ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ê°€ ë„ˆë¬´ í”ë“¤ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì™„ì¶©ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì¶©ê²© í¡ìˆ˜ê°€ ì œëŒ€ë¡œ ì•ˆë©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì™„ì¶©ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì„œìŠ¤íœì…˜ ì†ŒìŒì´ ì‹¬í•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì™„ì¶©ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì™„ì¶©ì¥ì¹˜ê°€ íŒŒì†ëìŠµë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì™„ì¶©ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íƒ€ì´ì–´ê°€ ë‹³ì•˜ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì£¼í–‰ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ê°€ í•œìª½ìœ¼ë¡œ ì ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì£¼í–‰ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì¡°í–¥ì´ ì œëŒ€ë¡œ ì•ˆë¼ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì£¼í–‰ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íœ ì´ í”ë“¤ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì£¼í–‰ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íƒ€ì´ì–´ ê³µê¸°ì••ì´ ìœ ì§€ë˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì£¼í–‰ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì œë™ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ íŒ¨ë“œê°€ ë‹³ì•˜ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì œë™ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ë¥¼ ë°Ÿìœ¼ë©´ ì†Œë¦¬ê°€ ë‚©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì œë™ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ê°€ ë°€ë¦½ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì œë™ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ ì˜¤ì¼ì´ ìƒˆìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì œë™ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—”ì§„ ì†Œë¦¬ê°€ ì´ìƒí•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì›ë™ê¸°\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—”ì§„ì´ ê³¼ì—´ë©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì›ë™ê¸°\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì‹œë™ì´ ì˜ ì•ˆê±¸ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì›ë™ê¸°\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—”ì§„ ì¶œë ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì›ë™ê¸°\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—”ì§„ì—ì„œ ì—°ê¸°ê°€ ë‚˜ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì›ë™ê¸°\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—°ë£Œê°€ ìƒˆìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì—°ë£Œì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—°ë£ŒíŒí”„ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì—°ë£Œì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—°ë£Œ ì†Œëª¨ê°€ ë„ˆë¬´ ë§ì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì—°ë£Œì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—°ë£Œíƒ±í¬ì— ë¬¸ì œê°€ ìˆì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì—°ë£Œì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—°ë£Œí•„í„°ê°€ ë§‰í˜”ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì—°ë£Œì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë³€ì†ê¸°ê°€ ê³ ì¥ë‚¬ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë³€ì†ì´ ì˜ ì•ˆë¼ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"í´ëŸ¬ì¹˜ê°€ ì´ìƒí•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë“œë¼ì´ë¸Œ ìƒ¤í”„íŠ¸ê°€ í”ë“¤ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë³€ì†ê¸° ì†ŒìŒì´ ì‹¬í•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë™ë ¥ì „ë‹¬ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë°°í„°ë¦¬ê°€ ë°©ì „ëì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì „ê¸°ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ê°€ ì‹œë™ì´ ì•ˆê±¸ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì „ê¸°ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì „ì¡°ë“±ì´ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì „ê¸°ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ê³„ê¸°íŒ ë¶ˆì´ ì•ˆë“¤ì–´ì™€ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì „ê¸°ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì™€ì´í¼ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì „ê¸°ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"í—¤ë“œë¼ì´íŠ¸ê°€ ê³ ì¥ë‚¬ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë“±í™”ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë°©í–¥ì§€ì‹œë“±ì´ ì•ˆë“¤ì–´ì™€ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë“±í™”ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë¸Œë ˆì´í¬ë“±ì´ ë‚˜ê°”ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë“±í™”ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì•ˆê°œë“±ì´ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë“±í™”ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"í›„ë¯¸ë“±ì´ ê¹œë¹¡ì—¬ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ë“±í™”ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì™€ì´í¼ê°€ ì•ˆë©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì›Œì…”ì•¡ì´ ì•ˆë‚˜ì˜µë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì™€ì´í¼ ì†Œë¦¬ê°€ ë‚©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì™€ì´í¼ê°€ ëœëœê±°ë¦½ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì „ë°© ìœ ë¦¬ ê¹€ì´ ì˜ ì•ˆì¡í˜€ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹œê³„í™•ë³´ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—ì–´ì»¨ì´ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íˆí„°ê°€ ì•ˆë‚˜ì˜µë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—ì–´ì»¨ ì†Œë¦¬ê°€ í½ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ëƒ‰ë°©ì´ ì˜ ì•ˆë©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë‚œë°©ì´ ì˜ ì•ˆë©ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ëƒ‰/ë‚œë°© ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì‹œíŠ¸ê°€ ì›€ì§ì´ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì•ˆì „ë²¨íŠ¸ê°€ ê³ ì¥ë‚¬ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°½ë¬¸ì´ ì•ˆì˜¬ë¼ê°€ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì¸í¬í…Œì¸ë¨¼íŠ¸ ì‹œìŠ¤í…œì´ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì‹œíŠ¸ íˆí„°ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ìŠ¹ì°¨ ë° ì‹¤ë‚´ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ì²´ì— ë…¹ì´ ìŠ¬ì—ˆì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°¨ì²´/ì°¨ëŒ€\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë„ì–´ê°€ ì œëŒ€ë¡œ ë‹«íˆì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°¨ì²´/ì°¨ëŒ€\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íŠ¸ë í¬ê°€ ì•ˆì—´ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°¨ì²´/ì°¨ëŒ€\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ì²´ ì†ŒìŒì´ ì‹¬í•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°¨ì²´/ì°¨ëŒ€\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë°”ë‹¥ì—ì„œ ë¬¼ì´ ìƒ™ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°¨ì²´/ì°¨ëŒ€\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"íŠ¸ë í¬ ì—´ì‡ ê°€ ê³ ì¥ë‚¬ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ê¸°íƒ€ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ë£¨í”„ë™ì´ í”ë“¤ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ê¸°íƒ€ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ëŸ‰ ê²½ê³ ìŒì´ ì•ˆìš¸ë ¤ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ê¸°íƒ€ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì „ë™ ë¯¸ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ê¸°íƒ€ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°¨ëŸ‰ ì‹œê³„ê°€ ì˜¤ì‘ë™í•´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ê¸°íƒ€ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—ì–´ë°± ê²½ê³ ë“±ì´ ì¼œì¡Œì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì•ˆì „ë²¨íŠ¸ í”„ë¦¬í…ì…”ë„ˆê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—ì–´ë°±ì´ ë¶€í’€ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì•ˆì „ë²¨íŠ¸ê°€ í—ê²ìŠµë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì—ì–´ë°± ê²½ê³ ë“±ì´ êº¼ì§€ì§€ ì•Šì•„ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì‹¤ë‚´ì•ˆì „ì¥ì¹˜\"},\n",
    "\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì „ë©´ ìœ ë¦¬ê°€ ê¸ˆì´ ê°”ì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°½ìœ ë¦¬\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"í›„ë©´ ìœ ë¦¬ê°€ ê¹¨ì¡Œì–´ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°½ìœ ë¦¬\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì¸¡ë©´ ìœ ë¦¬ê°€ ì˜ ì•ˆì˜¬ë¼ê°€ìš”\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°½ìœ ë¦¬\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ìœ ë¦¬ì— ê¹€ì´ ìì£¼ ìƒê¹ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°½ìœ ë¦¬\"},\n",
    "    {\"ê²°í•¨ë‚´ìš©\": \"ì°½ë¬¸ì—ì„œ ë¬¼ì´ ìƒ™ë‹ˆë‹¤\", \"ì¥ì¹˜ë¶„ë¥˜\": \"ì°½ìœ ë¦¬\"},\n",
    "]\n",
    "new_df = pd.DataFrame(data)\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ì¶”ê°€\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "# 2. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')\n",
    "model = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')\n",
    "\n",
    "# 3. Mean Pooling í•¨ìˆ˜ ì •ì˜\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# 4. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "encoded_input = tokenizer(df['ê²°í•¨ë‚´ìš©'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# 5. Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings.numpy(), df['ì¥ì¹˜ë¶„ë¥˜'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. ëª¨ë¸ í•™ìŠµ\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 7. ëª¨ë¸ í‰ê°€\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
